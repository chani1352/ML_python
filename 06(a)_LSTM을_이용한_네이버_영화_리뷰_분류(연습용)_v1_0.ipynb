{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "USE_CUDA = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
        "print(f\"다음 기기로 학습 : {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kqq2xWfmYHn2",
        "outputId": "3c793853-37a3-4002-d18c-0f263abaf592"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: konlpy in c:\\users\\user\\anaconda3\\envs\\torch_book\\lib\\site-packages (0.6.0)\n",
            "Requirement already satisfied: JPype1>=0.7.0 in c:\\users\\user\\anaconda3\\envs\\torch_book\\lib\\site-packages (from konlpy) (1.5.1)\n",
            "Requirement already satisfied: lxml>=4.1.0 in c:\\users\\user\\anaconda3\\envs\\torch_book\\lib\\site-packages (from konlpy) (5.3.0)\n",
            "Requirement already satisfied: numpy>=1.6 in c:\\users\\user\\anaconda3\\envs\\torch_book\\lib\\site-packages (from konlpy) (1.26.4)\n",
            "Requirement already satisfied: packaging in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from JPype1>=0.7.0->konlpy) (24.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install konlpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "qpN8-KIAYAe_"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import urllib.request\n",
        "from konlpy.tag import Okt\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53rVuC0sYEsC",
        "outputId": "4e09a15f-879d-43f5-cfce-4681d9c6aa85"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('ratings_test.txt', <http.client.HTTPMessage at 0x1696b384550>)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\", filename=\"ratings_train.txt\")\n",
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\", filename=\"ratings_test.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "MhKLy7cuYPVz"
      },
      "outputs": [],
      "source": [
        "train_data = pd.read_table(\"ratings_train.txt\")\n",
        "test_data = pd.read_table(\"ratings_test.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "VrwVs6P0YVhs"
      },
      "outputs": [],
      "source": [
        "train_data.drop_duplicates(subset=[\"document\"], inplace=True)\n",
        "train_data[\"document\"] = train_data[\"document\"].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\", regex=True)\n",
        "train_data[\"document\"] = train_data[\"document\"].str.replace(\"^ +\", \"\", regex=True)\n",
        "train_data[\"document\"] = train_data[\"document\"].replace(\"\", np.nan)\n",
        "train_data = train_data.dropna(how = \"any\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "j1lk42RxY5KX"
      },
      "outputs": [],
      "source": [
        "test_data.drop_duplicates(subset = [\"document\"], inplace=True)\n",
        "test_data[\"document\"] = test_data[\"document\"].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\", regex=True)\n",
        "test_data[\"document\"] = test_data[\"document\"].str.replace(\"^ +\", \"\", regex=True)\n",
        "test_data[\"document\"] = test_data[\"document\"].replace(\"\", np.nan)\n",
        "test_data = test_data.dropna(how=\"any\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "baMKPYAZZ0st",
        "outputId": "02c9c33c-111f-43f4-e386-e9985fb4e099"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "전처리 샘플의 개수 : 145393 48852\n"
          ]
        }
      ],
      "source": [
        "print('전처리 샘플의 개수 :',len(train_data),len(test_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ierYosozaG4e",
        "outputId": "81c1a20e-fcd1-4673-8ea1-3ab27396be93"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 145393/145393 [03:49<00:00, 632.26it/s]\n"
          ]
        }
      ],
      "source": [
        "stopwords = ['도', '는', '다', '의', '가', '이', '은', '한', '에', '하', '고', '을', '를', '인', '듯', '과', '와', '네', '들', '듯', '지', '임', '게']\n",
        "okt = Okt()\n",
        "X_train = []\n",
        "for sentence in tqdm(train_data['document']):\n",
        "    tokenized_sentence = okt.morphs(sentence)\n",
        "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords]\n",
        "    X_train.append(stopwords_removed_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "0gBhKCn6b9fL"
      },
      "outputs": [],
      "source": [
        "with open('X_train.pickle', 'wb') as f:\n",
        "    pickle.dump(X_train, f, pickle.HIGHEST_PROTOCOL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3vA4ay0Da2K1",
        "outputId": "23000a5b-944a-424f-dcd2-d9aeeb54ed3a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 48852/48852 [01:27<00:00, 555.61it/s]\n"
          ]
        }
      ],
      "source": [
        "X_test = []\n",
        "for sentence in tqdm(test_data['document']):\n",
        "    tokenized_sentence = okt.morphs(sentence) # 토큰화\n",
        "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\n",
        "    X_test.append(stopwords_removed_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "64sZJrZ_cFQs"
      },
      "outputs": [],
      "source": [
        "with open('X_test.pickle', 'wb') as f:\n",
        "    pickle.dump(X_test, f, pickle.HIGHEST_PROTOCOL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "AHOoFYjpchmC"
      },
      "outputs": [],
      "source": [
        "y_train = np.array(train_data['label'])\n",
        "y_test = np.array(test_data['label'])\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=0, stratify=y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "s_73rzLrcoIz"
      },
      "outputs": [],
      "source": [
        "word_list = []\n",
        "for sent in X_train:\n",
        "    for word in sent:\n",
        "      word_list.append(word)\n",
        "word_counts = Counter(word_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WyaMGb6nc2Lu",
        "outputId": "25ce8618-2699-4abd-c120-c8388adc28b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "총 단어수 : 88276\n",
            "훈련 데이터에서의 단어 영화의 등장 횟수 : 40264\n",
            "훈련 데이터에서의 단어 공감의 등장 횟수 : 784\n"
          ]
        }
      ],
      "source": [
        "print('총 단어수 :', len(word_counts))\n",
        "print('훈련 데이터에서의 단어 영화의 등장 횟수 :', word_counts['영화'])\n",
        "print('훈련 데이터에서의 단어 공감의 등장 횟수 :', word_counts['공감'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08Hn2HWac5kY",
        "outputId": "6f7c3074-7ec7-4085-acd9-6a0e5f1fb1c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "등장 빈도수 상위 10개 단어\n",
            "['영화', '너무', '정말', '만', '적', '진짜', '으로', '로', '점', '에서']\n"
          ]
        }
      ],
      "source": [
        "vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
        "print('등장 빈도수 상위 10개 단어')\n",
        "print(vocab[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QCr1e6d7c8nV",
        "outputId": "1b9c1ba9-4456-4064-d228-a00c87a2e041"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "단어 집합(vocabulary)의 크기 : 88276\n",
            "등장 빈도가 2번 이하인 희귀 단어의 수: 60078\n",
            "단어 집합에서 희귀 단어의 비율: 68.05700303593277\n",
            "전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 5.644509532507002\n"
          ]
        }
      ],
      "source": [
        "threshold = 3\n",
        "total_cnt = len(word_counts) # 단어의 수\n",
        "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
        "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
        "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
        "\n",
        "for key, value in word_counts.items():\n",
        "    total_freq = total_freq + value\n",
        "    if(value < threshold):\n",
        "        rare_cnt = rare_cnt + 1\n",
        "        rare_freq = rare_freq + value\n",
        "\n",
        "print('단어 집합(vocabulary)의 크기 :',total_cnt)\n",
        "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
        "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
        "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWe39MXRdA_1",
        "outputId": "6712a260-bcc5-4c00-c1d5-ec19314f2022"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "단어 집합의 크기 : 28198\n"
          ]
        }
      ],
      "source": [
        "# 전체 단어 개수 중 빈도수 2이하인 단어는 제거.\n",
        "vocab_size = total_cnt - rare_cnt\n",
        "vocab = vocab[:vocab_size]\n",
        "print('단어 집합의 크기 :', len(vocab))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EmJTnbbOdFE1",
        "outputId": "e91a61ec-8a2d-4ed4-e61d-ba5d118047c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "패딩 토큰과 UNK 토큰을 고려한 단어 집합의 크기 : 28200\n",
            "단어 <PAD>와 맵핑되는 정수 : 0\n",
            "단어 <UNK>와 맵핑되는 정수 : 1\n",
            "단어 영화와 맵핑되는 정수 : 2\n"
          ]
        }
      ],
      "source": [
        "word_to_index = {}\n",
        "word_to_index['<PAD>'] = 0\n",
        "word_to_index['<UNK>'] = 1\n",
        "for index, word in enumerate(vocab) :\n",
        "  word_to_index[word] = index + 2\n",
        "vocab_size = len(word_to_index)\n",
        "print('패딩 토큰과 UNK 토큰을 고려한 단어 집합의 크기 :', vocab_size)\n",
        "print('단어 <PAD>와 맵핑되는 정수 :', word_to_index['<PAD>'])\n",
        "print('단어 <UNK>와 맵핑되는 정수 :', word_to_index['<UNK>'])\n",
        "print('단어 영화와 맵핑되는 정수 :', word_to_index['영화'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "bFp3suUrdSC5"
      },
      "outputs": [],
      "source": [
        "def texts_to_sequences(tokenized_X_data, word_to_index):\n",
        "  encoded_X_data = []\n",
        "  for sent in tokenized_X_data:\n",
        "    index_sequences = []\n",
        "    for word in sent:\n",
        "      try:\n",
        "          index_sequences.append(word_to_index[word])\n",
        "      except KeyError:\n",
        "          index_sequences.append(word_to_index['<UNK>'])\n",
        "    encoded_X_data.append(index_sequences)\n",
        "  return encoded_X_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "XuDFlooIdUvM"
      },
      "outputs": [],
      "source": [
        "encoded_X_train = texts_to_sequences(X_train, word_to_index)\n",
        "encoded_X_valid = texts_to_sequences(X_valid, word_to_index)\n",
        "encoded_X_test = texts_to_sequences(X_test, word_to_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cupjBDwNdX5_",
        "outputId": "3565dfea-5b11-460c-fc80-e1c795a650a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "기존의 첫번째 샘플 : ['이야', '어쩜', '이렇게나', '지루할수가']\n",
            "복원된 첫번째 샘플 : ['이야', '어쩜', '이렇게나', '지루할수가']\n"
          ]
        }
      ],
      "source": [
        "index_to_word = {}\n",
        "for key, value in word_to_index.items():\n",
        "    index_to_word[value] = key\n",
        "decoded_sample = [index_to_word[word] for word in encoded_X_train[0]]\n",
        "print('기존의 첫번째 샘플 :', X_train[0])\n",
        "print('복원된 첫번째 샘플 :', decoded_sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0wffyGqUdgjZ",
        "outputId": "a475f849-11d3-4d79-8229-96def49d26ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "훈련 데이터의 크기 : (116314, 30)\n",
            "검증 데이터의 크기 : (29079, 30)\n",
            "테스트 데이터의 크기 : (48852, 30)\n"
          ]
        }
      ],
      "source": [
        "max_len = 30\n",
        "def pad_sequences(sentences, max_len):\n",
        "  features = np.zeros((len(sentences), max_len), dtype=int)\n",
        "  for index, sentence in enumerate(sentences):\n",
        "    if len(sentence) != 0:\n",
        "      features[index, :len(sentence)] = np.array(sentence)[:max_len]\n",
        "  return features\n",
        "padded_X_train = pad_sequences(encoded_X_train, max_len=max_len)\n",
        "padded_X_valid = pad_sequences(encoded_X_valid, max_len=max_len)\n",
        "padded_X_test = pad_sequences(encoded_X_test, max_len=max_len)\n",
        "print('훈련 데이터의 크기 :', padded_X_train.shape)\n",
        "print('검증 데이터의 크기 :', padded_X_valid.shape)\n",
        "print('테스트 데이터의 크기 :', padded_X_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXcHYXaTdqfF"
      },
      "source": [
        "## LSTM을 사용한 감정 분석"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "RyjGmFUNdtgb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07u_C0cidwDl",
        "outputId": "c2ba6363-c383-49a7-86f8-443f099e4b69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "다음 기기로 학습 : cuda\n"
          ]
        }
      ],
      "source": [
        "USE_CUDA = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
        "print(f\"다음 기기로 학습 : {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AvKQdAu0d2eq",
        "outputId": "2a6fcb3c-171a-4575-edc8-63226a687f72"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([0, 1, 1, 1, 1])\n"
          ]
        }
      ],
      "source": [
        "train_label_tensor = torch.tensor(np.array(y_train))\n",
        "valid_label_tensor = torch.tensor(np.array(y_valid))\n",
        "test_label_tensor = torch.tensor(np.array(y_test))\n",
        "print(train_label_tensor[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "yzts8e8GfscA"
      },
      "outputs": [],
      "source": [
        "\n",
        "# - 문장 길이 = 500\n",
        "# - 배치 크기 = 32\n",
        "# - 데이터 개수 = 2만\n",
        "\n",
        "# - LSTM 매개변수\n",
        "  # - 단어 벡터의 차원 = 100\n",
        "  # - LSTM의 은닉층의 크기 = 128\n",
        "  # - 분류하고자 하는 카테고리 개수 = 2개"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dN3SstcSiuIJ"
      },
      "source": [
        "### 텍스트 분류를 위한 레이어"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "uw7NjHgufybC"
      },
      "outputs": [],
      "source": [
        "class TextClassifier(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
        "    super(TextClassifier, self).__init__()\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "    self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
        "    self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    embedded = self.embedding(x)\n",
        "    lstm_put, (hidden, cell) = self.lstm(embedded)\n",
        "    last_hidden = hidden.squeeze(0)\n",
        "    logits = self.fc(last_hidden)\n",
        "    return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46WCmtAoizVD"
      },
      "source": [
        "### 데이터를 텐서로 변경"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "BVkiq9XZguPu"
      },
      "outputs": [],
      "source": [
        "encoded_train = torch.tensor(padded_X_train).to(torch.int64)\n",
        "train_dataset = torch.utils.data.TensorDataset(encoded_train, train_label_tensor)  #문제와 정답지로 분류\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, shuffle=True, batch_size=32)\n",
        "\n",
        "encoded_valid = torch.tensor(padded_X_valid).to(torch.int64)\n",
        "valid_dataset = torch.utils.data.TensorDataset(encoded_valid, valid_label_tensor)  #문제와 정답지로 분류\n",
        "valid_dataloader = torch.utils.data.DataLoader(valid_dataset, shuffle=True, batch_size=32)\n",
        "\n",
        "encoded_test = torch.tensor(padded_X_test).to(torch.int64)\n",
        "test_dataset = torch.utils.data.TensorDataset(encoded_test, test_label_tensor)  #문제와 정답지로 분류\n",
        "test_dataloader = torch.utils.data.DataLoader(test_dataset, shuffle=True, batch_size=32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYqwC83di4GX"
      },
      "source": [
        "### 배치데이터 구축"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBqmnAizgy_u",
        "outputId": "ce8e10e8-45b5-4c46-b508-987f0d0b048d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3635\n"
          ]
        }
      ],
      "source": [
        "total_batch = len(train_dataloader)\n",
        "print(total_batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhFV7w5_i7c1"
      },
      "source": [
        "### 모델 생성"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ymu-WagSg1WT",
        "outputId": "72790985-eac0-469b-c3a1-86d2a8f0a113"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TextClassifier(\n",
              "  (embedding): Embedding(28200, 100)\n",
              "  (lstm): LSTM(100, 128, batch_first=True)\n",
              "  (fc): Linear(in_features=128, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embedding_dim = 100\n",
        "hidden_dim = 128\n",
        "output_dim = 2\n",
        "learning_rate = 0.01\n",
        "num_epochs = 10\n",
        "\n",
        "model = TextClassifier(vocab_size, embedding_dim, hidden_dim, output_dim)\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPT49bx7i-Qh"
      },
      "source": [
        "### optimizer 등 설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "kdNpkJ_Ug4S9"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHzAOq7NjBge"
      },
      "source": [
        "### 정확도 계산"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "EfJ19IVGg7L4"
      },
      "outputs": [],
      "source": [
        "def cacluate_accuracy(logits, labels):\n",
        "  predicted = torch.argmax(logits, dim=1)\n",
        "  correct = (predicted == labels).sum().item()\n",
        "  total = labels.size(0)\n",
        "  accuracy = correct / total\n",
        "  return accuracy "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMxfD0HQjDPk"
      },
      "source": [
        "### 학습 및 계산"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "08rQUG31g-dx"
      },
      "outputs": [],
      "source": [
        "#학습할때 > 모델,valid_data, criterion(loss-function), device\n",
        "def evlaute(model, valid_dataloader, criterion, device):\n",
        "  val_loss = 0\n",
        "  val_correct = 0\n",
        "  val_total = 0\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for batch_X, batch_y in valid_dataloader:\n",
        "      batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "      logits = model(batch_X)\n",
        "      loss = criterion(logits, batch_y)\n",
        "      val_loss += loss.item()\n",
        "      val_correct += cacluate_accuracy(logits, batch_y) * batch_y.size(0)\n",
        "      val_total += batch_y.size(0)\n",
        "  val_accuracy = val_correct / val_total\n",
        "  val_loss /= len(valid_dataset)\n",
        "  return val_loss, val_accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-ZFL1aSjQ3G"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qt20qA7ChFKx",
        "outputId": "cbba5649-430b-4f70-e461-aab9bf987005"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch : 1 / 10\n",
            "Train Loss : 0.17349517812755116 / Train Accuracy : 0.5032841569517521\n",
            "Valid Loss : 0.021563869211933564 / Valid Accuracy : 0.5107465868840056\n",
            "Epoch : 2 / 10\n",
            "Train Loss : 0.14724115469432764 / Train Accuracy : 0.6751607689397847\n",
            "Valid Loss : 0.013211023508672263 / Valid Accuracy : 0.816671825028371\n",
            "Epoch : 3 / 10\n",
            "Train Loss : 0.09963692655015681 / Train Accuracy : 0.8222772447470683\n",
            "Valid Loss : 0.009337355079794033 / Valid Accuracy : 0.8821486295952405\n",
            "Epoch : 4 / 10\n",
            "Train Loss : 0.07436489161344802 / Train Accuracy : 0.8776092712954365\n",
            "Valid Loss : 0.006837850340934669 / Valid Accuracy : 0.9205612297534304\n",
            "Epoch : 5 / 10\n",
            "Train Loss : 0.054911961547897147 / Train Accuracy : 0.9132707452113209\n",
            "Valid Loss : 0.004727285914974985 / Valid Accuracy : 0.9480724921764847\n"
          ]
        }
      ],
      "source": [
        "best_val_loss = float('inf')\n",
        "#for epoch in range(num_epochs):\n",
        "for epoch in range(5):\n",
        "  train_loss = 0\n",
        "  train_correct = 0\n",
        "  train_total = 0\n",
        "  model.train()\n",
        "  for batch_X, batch_y in valid_dataloader:\n",
        "    batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "    logits = model(batch_X)\n",
        "    loss = criterion(logits, batch_y)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    train_loss += loss.item()\n",
        "    train_correct += cacluate_accuracy(logits, batch_y) * batch_y.size(0)\n",
        "    train_total += batch_y.size(0)\n",
        "  train_accuracy = train_correct / train_total\n",
        "  train_loss /= len(train_dataloader)\n",
        "  val_loss, val_accuracy = evlaute(model, valid_dataloader, criterion, device)\n",
        "  print(f\"Epoch : {epoch + 1} / {num_epochs}\")\n",
        "  print(f\"Train Loss : {train_loss} / Train Accuracy : {train_accuracy}\")\n",
        "  print(f\"Valid Loss : {val_loss} / Valid Accuracy : {val_accuracy}\")\n",
        "  if val_loss < best_val_loss:\n",
        "    best_val_loss = val_loss\n",
        "    torch.save(model.state_dict(), \"best_model_checkpoint.pt\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELf6pHHxjR5F"
      },
      "source": [
        "### 모델 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKGuxDSIhOTo",
        "outputId": "0ced272d-59a3-4bed-c446-5b03c1b2a2b3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71DPSUWgjVyg"
      },
      "source": [
        "### 측정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4KLAMDerhVFv",
        "outputId": "22e4149a-d153-42a8-8ab5-87c7c9e3d569"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "torch_book",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
